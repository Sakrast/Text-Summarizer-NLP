{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-21T11:02:46.773210Z","iopub.execute_input":"2024-02-21T11:02:46.773559Z","iopub.status.idle":"2024-02-21T11:02:47.932084Z","shell.execute_reply.started":"2024-02-21T11:02:46.773532Z","shell.execute_reply":"2024-02-21T11:02:47.931199Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:02:47.934059Z","iopub.execute_input":"2024-02-21T11:02:47.934516Z","iopub.status.idle":"2024-02-21T11:02:49.041727Z","shell.execute_reply.started":"2024-02-21T11:02:47.934481Z","shell.execute_reply":"2024-02-21T11:02:49.040539Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Wed Feb 21 11:02:48 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade accelerate\n!pip uninstall -y transformers accelerate\n!pip install transformers accelerate","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:02:49.043978Z","iopub.execute_input":"2024-02-21T11:02:49.044359Z","iopub.status.idle":"2024-02-21T11:03:28.349055Z","shell.execute_reply.started":"2024-02-21T11:02:49.044323Z","shell.execute_reply":"2024-02-21T11:03:28.348131Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.26.1)\nCollecting accelerate\n  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.26.1\n    Uninstalling accelerate-0.26.1:\n      Successfully uninstalled accelerate-0.26.1\nSuccessfully installed accelerate-0.27.2\nFound existing installation: transformers 4.37.0\nUninstalling transformers-4.37.0:\n  Successfully uninstalled transformers-4.37.0\nFound existing installation: accelerate 0.27.2\nUninstalling accelerate-0.27.2:\n  Successfully uninstalled accelerate-0.27.2\nCollecting transformers\n  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate\n  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hUsing cached accelerate-0.27.2-py3-none-any.whl (279 kB)\nInstalling collected packages: accelerate, transformers\nSuccessfully installed accelerate-0.27.2 transformers-4.37.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom datasets import load_dataset, load_from_disk\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nfrom tqdm import tqdm\nimport torch\n\nnltk.download(\"punkt\")","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:03:28.351825Z","iopub.execute_input":"2024-02-21T11:03:28.352115Z","iopub.status.idle":"2024-02-21T11:03:51.472612Z","shell.execute_reply.started":"2024-02-21T11:03:28.352088Z","shell.execute_reply":"2024-02-21T11:03:51.471602Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-02-21 11:03:36.510336: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-21 11:03:36.510464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-21 11:03:36.688491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install py7zr\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:03:51.473956Z","iopub.execute_input":"2024-02-21T11:03:51.475127Z","iopub.status.idle":"2024-02-21T11:04:15.320874Z","shell.execute_reply.started":"2024-02-21T11:03:51.475091Z","shell.execute_reply":"2024-02-21T11:04:15.319652Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting py7zr\n  Downloading py7zr-0.20.8-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.7.0)\nCollecting pycryptodomex>=3.16.0 (from py7zr)\n  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting pyzstd>=0.15.9 (from py7zr)\n  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\nCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting brotli>=1.1.0 (from py7zr)\n  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nDownloading py7zr-0.20.8-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n  Attempting uninstall: brotli\n    Found existing installation: Brotli 1.0.9\n    Uninstalling Brotli-1.0.9:\n      Successfully uninstalled Brotli-1.0.9\nSuccessfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.20.8 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.9\n","output_type":"stream"}]},{"cell_type":"code","source":"import py7zr","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:15.322418Z","iopub.execute_input":"2024-02-21T11:04:15.322740Z","iopub.status.idle":"2024-02-21T11:04:15.443170Z","shell.execute_reply.started":"2024-02-21T11:04:15.322712Z","shell.execute_reply":"2024-02-21T11:04:15.442441Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:15.444205Z","iopub.execute_input":"2024-02-21T11:04:15.444534Z","iopub.status.idle":"2024-02-21T11:04:15.448929Z","shell.execute_reply.started":"2024-02-21T11:04:15.444506Z","shell.execute_reply":"2024-02-21T11:04:15.447837Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:15.450245Z","iopub.execute_input":"2024-02-21T11:04:15.450846Z","iopub.status.idle":"2024-02-21T11:04:15.510814Z","shell.execute_reply.started":"2024-02-21T11:04:15.450814Z","shell.execute_reply":"2024-02-21T11:04:15.509663Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:15.512042Z","iopub.execute_input":"2024-02-21T11:04:15.512380Z","iopub.status.idle":"2024-02-21T11:04:15.520827Z","shell.execute_reply.started":"2024-02-21T11:04:15.512345Z","shell.execute_reply":"2024-02-21T11:04:15.519771Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_ckpt = \"google/pegasus-cnn_dailymail\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:15.525272Z","iopub.execute_input":"2024-02-21T11:04:15.525690Z","iopub.status.idle":"2024-02-21T11:04:18.088370Z","shell.execute_reply.started":"2024-02-21T11:04:15.525662Z","shell.execute_reply":"2024-02-21T11:04:18.087332Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87c0090cbfd4c7b98310e2f7a35ed3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b81ca2b8084820a71ddcf1f4bc1e06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f548ce643c0c42b88851b59ea8cafe94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"365278628915492090812f9001ef4118"}},"metadata":{}}]},{"cell_type":"code","source":"model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) # used to automatically load sequence-to-sequence language model","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:18.089758Z","iopub.execute_input":"2024-02-21T11:04:18.090138Z","iopub.status.idle":"2024-02-21T11:04:40.669818Z","shell.execute_reply.started":"2024-02-21T11:04:18.090103Z","shell.execute_reply":"2024-02-21T11:04:40.668984Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d7130b58a54df1af26ef1d75d435aa"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead7f9bcb862494e832c6880a97c07ce"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_samsum = load_dataset(\"samsum\")","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:40.671051Z","iopub.execute_input":"2024-02-21T11:04:40.671441Z","iopub.status.idle":"2024-02-21T11:04:44.134033Z","shell.execute_reply.started":"2024-02-21T11:04:40.671405Z","shell.execute_reply":"2024-02-21T11:04:44.133150Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8069a05a399643c7bcd1884e3a719bc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/770 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d66296c960499fb7f1dbc7df3fbdd0"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"275a8955d484444fa9289c9921b19fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f86bd7d7325484391c59c2c70a0b459"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_samsum","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:44.135165Z","iopub.execute_input":"2024-02-21T11:04:44.135527Z","iopub.status.idle":"2024-02-21T11:04:44.142254Z","shell.execute_reply.started":"2024-02-21T11:04:44.135495Z","shell.execute_reply":"2024-02-21T11:04:44.141345Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n\n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n\n    return {\n        'input_ids' : input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': target_encodings['input_ids']\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:44.143560Z","iopub.execute_input":"2024-02-21T11:04:44.144132Z","iopub.status.idle":"2024-02-21T11:04:44.154069Z","shell.execute_reply.started":"2024-02-21T11:04:44.144069Z","shell.execute_reply":"2024-02-21T11:04:44.153227Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:44.155380Z","iopub.execute_input":"2024-02-21T11:04:44.155963Z","iopub.status.idle":"2024-02-21T11:04:49.246965Z","shell.execute_reply.started":"2024-02-21T11:04:44.155932Z","shell.execute_reply":"2024-02-21T11:04:49.246179Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88427c912b8844b4b5b9af415ea13801"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"345acba787c8480c9be7468d38ced1be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e3f9b56ade4ba5bad2c2714aac9e43"}},"metadata":{}}]},{"cell_type":"code","source":"# Training\n\nfrom transformers import DataCollatorForSeq2Seq\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:49.248107Z","iopub.execute_input":"2024-02-21T11:04:49.248452Z","iopub.status.idle":"2024-02-21T11:04:49.253398Z","shell.execute_reply.started":"2024-02-21T11:04:49.248421Z","shell.execute_reply":"2024-02-21T11:04:49.252152Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntrainer_args = TrainingArguments(\n    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n    per_device_train_batch_size=1, per_device_eval_batch_size=10,\n    weight_decay=0.01, logging_steps=10,\n    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n    gradient_accumulation_steps=16\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:49.254615Z","iopub.execute_input":"2024-02-21T11:04:49.254986Z","iopub.status.idle":"2024-02-21T11:04:49.284669Z","shell.execute_reply.started":"2024-02-21T11:04:49.254955Z","shell.execute_reply":"2024-02-21T11:04:49.283948Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\ntrainer = Trainer(model=model_pegasus, args=trainer_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"],\n                  eval_dataset=dataset_samsum_pt[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:49.285625Z","iopub.execute_input":"2024-02-21T11:04:49.285881Z","iopub.status.idle":"2024-02-21T11:04:50.256485Z","shell.execute_reply.started":"2024-02-21T11:04:49.285859Z","shell.execute_reply":"2024-02-21T11:04:50.255716Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:04:50.257632Z","iopub.execute_input":"2024-02-21T11:04:50.258001Z","iopub.status.idle":"2024-02-21T12:25:27.865150Z","shell.execute_reply.started":"2024-02-21T11:04:50.257967Z","shell.execute_reply":"2024-02-21T12:25:27.864172Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240221_110512-6l6mb3t2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sarthakrastogi2021/huggingface/runs/6l6mb3t2' target=\"_blank\">glowing-horse-3</a></strong> to <a href='https://wandb.ai/sarthakrastogi2021/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sarthakrastogi2021/huggingface' target=\"_blank\">https://wandb.ai/sarthakrastogi2021/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sarthakrastogi2021/huggingface/runs/6l6mb3t2' target=\"_blank\">https://wandb.ai/sarthakrastogi2021/huggingface/runs/6l6mb3t2</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [460/460 1:19:30, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=460, training_loss=2.010979511426843, metrics={'train_runtime': 4837.2431, 'train_samples_per_second': 3.046, 'train_steps_per_second': 0.095, 'total_flos': 7685907619971072.0, 'train_loss': 2.010979511426843, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluation\n\ndef generate_batch_sized_chunks(list_of_elements, batch_size):\n    \"\"\"split the dataset into smaller batches that we can process simultaneously\n    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]\n\n\n\ndef calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n                               batch_size=16, device=device,\n                               column_text=\"article\",\n                               column_summary=\"highlights\"):\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n\n        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n                        padding=\"max_length\", return_tensors=\"pt\")\n\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                         attention_mask=inputs[\"attention_mask\"].to(device),\n                         length_penalty=0.8, num_beams=8, max_length=128)\n        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n\n        # Finally, we decode the generated texts,\n        # replace the  token, and add the decoded texts with the references to the metric.\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n                                clean_up_tokenization_spaces=True)\n               for s in summaries]\n\n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n\n\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n\n    #  Finally compute and return the ROUGE scores.\n    score = metric.compute()\n    return score\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:25:27.867371Z","iopub.execute_input":"2024-02-21T12:25:27.867648Z","iopub.status.idle":"2024-02-21T12:25:27.882780Z","shell.execute_reply.started":"2024-02-21T12:25:27.867624Z","shell.execute_reply":"2024-02-21T12:25:27.881527Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:49:47.773763Z","iopub.execute_input":"2024-02-21T12:49:47.774208Z","iopub.status.idle":"2024-02-21T12:50:02.954633Z","shell.execute_reply.started":"2024-02-21T12:49:47.774156Z","shell.execute_reply":"2024-02-21T12:50:02.953460Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c19ef897d4bc210ff08c84d0c3935cc8f6b840c9b2a7a8f09f225bd87451b54b\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:50:30.064571Z","iopub.execute_input":"2024-02-21T12:50:30.065006Z","iopub.status.idle":"2024-02-21T12:50:30.077560Z","shell.execute_reply.started":"2024-02-21T12:50:30.064972Z","shell.execute_reply":"2024-02-21T12:50:30.076346Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrouge_metric = load_metric('rouge')","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:50:33.499192Z","iopub.execute_input":"2024-02-21T12:50:33.500067Z","iopub.status.idle":"2024-02-21T12:50:34.059259Z","shell.execute_reply.started":"2024-02-21T12:50:33.500034Z","shell.execute_reply":"2024-02-21T12:50:34.058192Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"score = calculate_metric_on_test_ds(\n    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue',\n    column_summary= 'summary'\n)\n\nrouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n\npd.DataFrame(rouge_dict, index = [f'pegasus'] )","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:50:35.573748Z","iopub.execute_input":"2024-02-21T12:50:35.574111Z","iopub.status.idle":"2024-02-21T12:50:45.513771Z","shell.execute_reply.started":"2024-02-21T12:50:35.574083Z","shell.execute_reply":"2024-02-21T12:50:45.512741Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"100%|| 5/5 [00:09<00:00,  1.93s/it]\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"           rouge1  rouge2    rougeL  rougeLsum\npegasus  0.022938     0.0  0.023009   0.022889","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pegasus</th>\n      <td>0.022938</td>\n      <td>0.0</td>\n      <td>0.023009</td>\n      <td>0.022889</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Save model\nmodel_pegasus.save_pretrained(\"pegasus-samsum-model\")","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:50:45.989048Z","iopub.execute_input":"2024-02-21T12:50:45.990006Z","iopub.status.idle":"2024-02-21T12:50:49.960796Z","shell.execute_reply.started":"2024-02-21T12:50:45.989972Z","shell.execute_reply":"2024-02-21T12:50:49.959571Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"## Save tokenizer\ntokenizer.save_pretrained(\"tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:50:49.962844Z","iopub.execute_input":"2024-02-21T12:50:49.963233Z","iopub.status.idle":"2024-02-21T12:50:50.006715Z","shell.execute_reply.started":"2024-02-21T12:50:49.963196Z","shell.execute_reply":"2024-02-21T12:50:50.005771Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/spiece.model',\n 'tokenizer/added_tokens.json',\n 'tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"#Load\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-02-21T13:02:17.725179Z","iopub.execute_input":"2024-02-21T13:02:17.726071Z","iopub.status.idle":"2024-02-21T13:02:17.878549Z","shell.execute_reply.started":"2024-02-21T13:02:17.726034Z","shell.execute_reply":"2024-02-21T13:02:17.877485Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#Prediction\n\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n\n\n\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n\nreference = dataset_samsum[\"test\"][0][\"summary\"]\n\npipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n\n##\nprint(\"Dialogue:\")\nprint(sample_text)\n\n\nprint(\"\\nReference Summary:\")\nprint(reference)\n\n\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-21T13:02:47.996490Z","iopub.execute_input":"2024-02-21T13:02:47.996870Z","iopub.status.idle":"2024-02-21T13:03:06.458100Z","shell.execute_reply.started":"2024-02-21T13:02:47.996842Z","shell.execute_reply":"2024-02-21T13:03:06.456986Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him \nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nReference Summary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n\nModel Summary:\nAmanda can't find Betty's number. Larry called Betty last time they were at the park together. Hannah wants Amanda to text Larry. Amanda will text Larry.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}